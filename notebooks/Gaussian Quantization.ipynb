{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torch.distributions as tdist\n",
    "import numpy as np\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Where to add a new import\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 1000\n",
    "n_sub_iters = 10\n",
    "batch_size = 1 \n",
    "num_atoms = 10\n",
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dist = tdist.MultivariateNormal(torch.tensor([0.]), torch.tensor([[1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_cost(x,y):\n",
    "    return torch.norm(x-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def er_chi_unnorm(x,yj,gj, epsilon, cost_func=l2_cost):\n",
    "    return torch.exp((-cost_func(x,yj)+gj)/epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def er_chi_j(x, y, g, epsilon, j):\n",
    "    numerator = er_chi_unnorm(x,y[j],g[j], epsilon)\n",
    "    print(numerator)\n",
    "    denominator = torch.sum(er_chi_unnorm(x,y,g, epsilon))\n",
    "    return numerator/ denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def er_chi(x, y, g, epsilon):\n",
    "    chis = er_chi_unnorm(x,y,g, epsilon)\n",
    "    normaliser = torch.sum(chis)\n",
    "    return chis/ normaliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropic reg c-transform\n",
    "def er_ctran(x, g, y, epsilon, cost_func):\n",
    "    return -epsilon * torch.log(torch.sum(torch.exp((-cost_func(x,y)+g)/epsilon))) + torch.sum(g, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init vectors\n",
    "y = torch.tensor(np.arange(10)/10., requires_grad = True)\n",
    "g = torch.tensor([np.random.random(size=num_atoms)], requires_grad = True)\n",
    "epsilon = torch.tensor(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTANTIATE OPTIMIZER CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer_atoms = torch.optim.SGD([y], lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "optimizer_map = torch.optim.SGD([g], lr=learning_rate, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTANTIATE STEP LEARNING SCHEDULER CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_size: at how many multiples of epoch you decay\n",
    "# new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler_atoms = StepLR(optimizer_atoms, step_size=1, gamma=0.8)\n",
    "scheduler_map = StepLR(optimizer_map, step_size=1, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6144, 0.7008, 1.0332, 0.3547, 0.9850, 1.0443, 0.4550, 1.0159, 0.6922,\n",
      "         0.4975]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-8.2268, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[67.4907, 67.4906, 67.4906, 67.4907, 67.4906, 67.4906, 67.4907, 67.4906,\n",
      "         67.4906, 67.4907]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-603.6753, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_iters):\n",
    "    x = normal_dist.sample()\n",
    "\n",
    "    # Clear gradients w.r.t. parameters\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    # Get dual objective to maximise\n",
    "    dual_objective = er_ctran(x, g, y, epsilon, l2_cost)\n",
    "    map_loss = -dual_objective\n",
    "\n",
    "    # Getting gradients w.r.t. parameters\n",
    "    map_loss.backward()\n",
    "    optimizer_map.step()\n",
    "    \n",
    "    # Updating parameters\n",
    "    if i % 100 == 0:\n",
    "        print(g)\n",
    "        print(map_loss)\n",
    "        scheduler_map.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Max Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: nan\n",
      "Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_iters):\n",
    "    for _ in range(n_sub_iters):\n",
    "        # sample x\n",
    "        x = normal_dist.sample()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer_map.zero_grad()\n",
    "        \n",
    "        # Get dual objective to maximise\n",
    "        dual_objective = er_ctran(x, g, y, epsilon, l2_cost)\n",
    "        map_loss = -dual_objective\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        map_loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer_map.step()\n",
    "        \n",
    "    for _ in range(n_sub_iters):\n",
    "        # sample x\n",
    "        x = normal_dist.sample()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer_atoms.zero_grad()\n",
    "        \n",
    "        # Get loss objective to minimise\n",
    "        atoms_loss = er_ctran(x, g, y, epsilon, l2_cost)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        atoms_loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer_atoms.step()\n",
    "    \n",
    "    # Updating parameters\n",
    "    if i % 100 == 0:\n",
    "        print(g)\n",
    "        print(map_loss)\n",
    "        # Decay Learning Rate\n",
    "        scheduler_atoms.step()\n",
    "        scheduler_map.step()\n",
    "    \n",
    "    print(\"Loss: {0}\".format(map_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=torch.float64,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
