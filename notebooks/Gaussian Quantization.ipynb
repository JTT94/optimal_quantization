{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torch.distributions as tdist\n",
    "import numpy as np\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Where to add a new import\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 1000\n",
    "n_sub_iters = 10\n",
    "batch_size = 1 \n",
    "num_atoms = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dist = tdist.MultivariateNormal(torch.tensor([0.]), torch.tensor([[1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_cost(x,y):\n",
    "    return torch.norm(x-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def er_chi_unnorm(x,yj,gj, epsilon, cost_func=l2_cost):\n",
    "    return torch.exp((-cost_func(x,yj)+gj)/epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def er_chi_j(x, y, g, epsilon, j):\n",
    "    numerator = er_chi_unnorm(x,y[j],g[j], epsilon)\n",
    "    print(numerator)\n",
    "    denominator = torch.sum(er_chi_unnorm(x,y,g, epsilon))\n",
    "    return numerator/ denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def er_chi(x, y, g, epsilon):\n",
    "    chis = er_chi_unnorm(x,y,g, epsilon)\n",
    "    normaliser = torch.sum(chis)\n",
    "    return chis/ normaliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropic reg c-transform\n",
    "def er_ctran(x, g, y, epsilon, cost_func):\n",
    "    return -epsilon * torch.log(torch.sum(torch.exp((-cost_func(x,y)+g)/epsilon))) + torch.sum(g)/ torch.tensor(num_atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init vectors\n",
    "y = torch.tensor(np.arange(10)/10., requires_grad = True)\n",
    "g = torch.tensor([np.random.random(size=num_atoms)], requires_grad = True)\n",
    "epsilon = torch.tensor(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0429, 0.0429, 0.0429, 0.0429, 0.0429, 0.0429, 0.0429, 0.0429, 0.0429,\n",
       "        0.0429], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTANTIATE OPTIMIZER CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer_atoms = torch.optim.SGD([y], lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "optimizer_map = torch.optim.SGD([g], lr=learning_rate, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTANTIATE STEP LEARNING SCHEDULER CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_size: at how many multiples of epoch you decay\n",
    "# new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler_atoms = StepLR(optimizer_atoms, step_size=1, gamma=0.8)\n",
    "scheduler_map = StepLR(optimizer_map, step_size=1, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3479, 0.1961, 0.7294, 0.1560, 0.4886, 0.1317, 0.7441, 0.0317, 0.5805,\n",
      "         0.2598]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-3.0419, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-2.7394, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-2.7701, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-3.0077, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-1.1555, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-5.8763, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-2.8752, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-3.2072, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-0.6863, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-2.5167, dtype=torch.float64, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_iters):\n",
    "    x = normal_dist.sample()\n",
    "\n",
    "    # Clear gradients w.r.t. parameters\n",
    "    optimizer_map.zero_grad()\n",
    "\n",
    "    # Get dual objective to maximise\n",
    "    dual_objective = er_ctran(x, g, y, epsilon, l2_cost)\n",
    "    map_loss = -dual_objective\n",
    "\n",
    "    # Getting gradients w.r.t. parameters\n",
    "    map_loss.backward()\n",
    "    optimizer_map.step()\n",
    "    \n",
    "    # Updating parameters\n",
    "    if i % 100 == 0:\n",
    "        print(g)\n",
    "        print(map_loss)\n",
    "        scheduler_map.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Max Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-2.3242, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: -2.3241907345162387\n",
      "Loss: -3.1829023809139434\n",
      "Loss: -2.6812323799178124\n",
      "Loss: -3.008164645047274\n",
      "Loss: -0.2701686462248475\n",
      "Loss: -0.6533732384174038\n",
      "Loss: -2.745527292332235\n",
      "Loss: -4.068370050707714\n",
      "Loss: -3.6785262578120967\n",
      "Loss: -0.3519530951295929\n",
      "Loss: -0.13929094718402488\n",
      "Loss: -0.49393664815895477\n",
      "Loss: -2.9691273010168313\n",
      "Loss: -0.3681468157132289\n",
      "Loss: -2.7036094815799285\n",
      "Loss: -1.5924728043746585\n",
      "Loss: -4.236500499143582\n",
      "Loss: -2.4857191267215266\n",
      "Loss: -4.054245175620577\n",
      "Loss: 0.08852705174950731\n",
      "Loss: -0.8619305474198184\n",
      "Loss: -1.510816624054502\n",
      "Loss: -0.45690866789379936\n",
      "Loss: -2.227293540485137\n",
      "Loss: -4.035310459354085\n",
      "Loss: -0.38637046403357117\n",
      "Loss: -3.1265627274282477\n",
      "Loss: -0.9678124335972247\n",
      "Loss: -1.1388815736926912\n",
      "Loss: -0.28657569678159117\n",
      "Loss: 0.17278693245011495\n",
      "Loss: -3.8158640093670377\n",
      "Loss: -2.1545484318053805\n",
      "Loss: -1.5232936046771341\n",
      "Loss: -4.4643191148990455\n",
      "Loss: 0.19550741360694496\n",
      "Loss: -0.4365125623007857\n",
      "Loss: -4.885068185310361\n",
      "Loss: -1.7669590412641107\n",
      "Loss: -2.0485432024288572\n",
      "Loss: -2.6093303077830923\n",
      "Loss: -0.20003240413896897\n",
      "Loss: -0.7242490913592142\n",
      "Loss: -0.8328803718451487\n",
      "Loss: -7.178863871973956\n",
      "Loss: -0.20792765600680044\n",
      "Loss: -2.6417514620799247\n",
      "Loss: -4.195507231108406\n",
      "Loss: -5.569095319936932\n",
      "Loss: -0.46806966682464674\n",
      "Loss: 0.05612939159736874\n",
      "Loss: -1.7728605611128696\n",
      "Loss: 0.22308981077449624\n",
      "Loss: -9.123469106451987\n",
      "Loss: -1.212049538253618\n",
      "Loss: -5.147216043039522\n",
      "Loss: -3.2238068420033503\n",
      "Loss: -4.299699285746196\n",
      "Loss: -0.7787840720718732\n",
      "Loss: -1.1307298079582984\n",
      "Loss: -2.5638163205791944\n",
      "Loss: -1.1011589740590153\n",
      "Loss: -3.377759187390968\n",
      "Loss: -2.840911200960319\n",
      "Loss: -6.047277038256852\n",
      "Loss: -2.261630482247338\n",
      "Loss: 0.160081395176829\n",
      "Loss: -2.1138550284247515\n",
      "Loss: -1.225395961185165\n",
      "Loss: -0.6679979444553491\n",
      "Loss: -5.700253916523225\n",
      "Loss: -3.8757092100744694\n",
      "Loss: -5.743762548247873\n",
      "Loss: -1.3910185273902056\n",
      "Loss: -2.2343271076431015\n",
      "Loss: -11.264637003311353\n",
      "Loss: -3.316408913704099\n",
      "Loss: -5.7746246820564195\n",
      "Loss: -1.3623089304049536\n",
      "Loss: -6.08935942370256\n",
      "Loss: -2.0844736049481014\n",
      "Loss: -4.740541102996417\n",
      "Loss: -0.9509843254993471\n",
      "Loss: -3.8752251402213758\n",
      "Loss: -0.10705297348924026\n",
      "Loss: -0.10116181777154076\n",
      "Loss: -0.17120383975214024\n",
      "Loss: -3.9274877955609226\n",
      "Loss: -6.989880821107948\n",
      "Loss: -6.75873649540029\n",
      "Loss: -0.31051440520993506\n",
      "Loss: -2.79334295097558\n",
      "Loss: -2.777273818755592\n",
      "Loss: -1.3570961139211655\n",
      "Loss: -2.2636411174019515\n",
      "Loss: -3.40578350312774\n",
      "Loss: -4.065811401784148\n",
      "Loss: -0.4106730215945405\n",
      "Loss: -2.0907478399241923\n",
      "Loss: -1.197258635016915\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-0.9164, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: -0.9163534127612472\n",
      "Loss: -2.782839069906214\n",
      "Loss: -2.139531242778995\n",
      "Loss: -2.361177658476362\n",
      "Loss: -0.39220458460047375\n",
      "Loss: -0.48024704568093496\n",
      "Loss: -0.08979346949770789\n",
      "Loss: -1.3983958753879686\n",
      "Loss: -8.992970416691586\n",
      "Loss: -1.9234178412828682\n",
      "Loss: -0.9618291557749366\n",
      "Loss: -1.0986069695868665\n",
      "Loss: 0.06914960988173491\n",
      "Loss: -1.7338199300741668\n",
      "Loss: -5.275885541347786\n",
      "Loss: -2.075301261480708\n",
      "Loss: -1.9993883201113258\n",
      "Loss: -0.49876454193187314\n",
      "Loss: -0.22785246900075262\n",
      "Loss: -3.8008964585169176\n",
      "Loss: -0.8508568273920072\n",
      "Loss: -6.050740699731103\n",
      "Loss: -5.715782026386784\n",
      "Loss: -3.7824688204306915\n",
      "Loss: -0.7856223749879114\n",
      "Loss: -2.150404555800302\n",
      "Loss: -2.2534735987755377\n",
      "Loss: -5.065626184530448\n",
      "Loss: -1.9701326151289926\n",
      "Loss: 0.11265507386064316\n",
      "Loss: -2.6274178314740237\n",
      "Loss: -6.55770939039227\n",
      "Loss: -2.7694618171682497\n",
      "Loss: -0.6697924081533592\n",
      "Loss: -2.280456859383726\n",
      "Loss: -1.43326343399171\n",
      "Loss: -2.1349897234033657\n",
      "Loss: -5.104422274589092\n",
      "Loss: -0.1982338093702637\n",
      "Loss: -2.9482010501815594\n",
      "Loss: 0.007009429320715854\n",
      "Loss: -3.8534661757804454\n",
      "Loss: -4.468986589828956\n",
      "Loss: -0.330855283003231\n",
      "Loss: -2.2563049077053003\n",
      "Loss: -3.794752888095378\n",
      "Loss: -0.12063025636260488\n",
      "Loss: -5.531906095579631\n",
      "Loss: -2.1883495206161747\n",
      "Loss: -3.1586703021821547\n",
      "Loss: -1.2511725867266978\n",
      "Loss: -0.0014820893866011042\n",
      "Loss: -1.852675064729884\n",
      "Loss: -1.5031382904213613\n",
      "Loss: -2.348575544044034\n",
      "Loss: -0.08642515756019764\n",
      "Loss: -0.5202557131995351\n",
      "Loss: -1.8758817388438866\n",
      "Loss: -0.05520812815906101\n",
      "Loss: -2.419875206584841\n",
      "Loss: -5.479788933057376\n",
      "Loss: -1.1870706908765392\n",
      "Loss: -2.635501435355306\n",
      "Loss: -2.6732025178601226\n",
      "Loss: -1.6115573395634306\n",
      "Loss: 0.19043231097142482\n",
      "Loss: -1.1266983912920403\n",
      "Loss: -0.44445886268570434\n",
      "Loss: -3.7881188032822815\n",
      "Loss: -0.597399809987551\n",
      "Loss: -1.2206849960356825\n",
      "Loss: -1.7994558383615875\n",
      "Loss: 0.1455814322366767\n",
      "Loss: -2.6767728153686767\n",
      "Loss: -0.4099521437027855\n",
      "Loss: -1.601990284641817\n",
      "Loss: -2.963982100013841\n",
      "Loss: -4.037290545478895\n",
      "Loss: -0.7256521796857012\n",
      "Loss: -5.5396912255413815\n",
      "Loss: -1.6075148825760939\n",
      "Loss: -0.7365330778973607\n",
      "Loss: -9.18542699442605\n",
      "Loss: -0.7096146011254101\n",
      "Loss: -0.6053665458851643\n",
      "Loss: -1.0588842737088133\n",
      "Loss: -7.469582273562446\n",
      "Loss: -6.467604922627612\n",
      "Loss: -4.968792511852873\n",
      "Loss: -2.403188543418627\n",
      "Loss: 0.1993843343463007\n",
      "Loss: -0.7970492079593129\n",
      "Loss: -2.1954808696590784\n",
      "Loss: -1.6200738454980808\n",
      "Loss: -5.8346188720469305\n",
      "Loss: -3.32491983459403\n",
      "Loss: -2.0999397022057655\n",
      "Loss: -3.0352981400450827\n",
      "Loss: -3.6665783916491645\n",
      "Loss: -1.1246171531720637\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-3.2472, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: -3.2471753286567693\n",
      "Loss: -0.12540206666237635\n",
      "Loss: -1.6097764001403756\n",
      "Loss: -0.0673604188026567\n",
      "Loss: -5.406208906223599\n",
      "Loss: -5.4016127073612115\n",
      "Loss: -0.2475394958271698\n",
      "Loss: -0.3920549486654196\n",
      "Loss: -0.38126433245308955\n",
      "Loss: -1.6569958233649986\n",
      "Loss: -2.536796969801982\n",
      "Loss: -1.136921334804029\n",
      "Loss: -6.262773649192929\n",
      "Loss: -3.8560840386245627\n",
      "Loss: -1.3174075137161778\n",
      "Loss: -1.6929808925030105\n",
      "Loss: -2.7089665177210493\n",
      "Loss: -3.619898471191621\n",
      "Loss: -3.3714988279437614\n",
      "Loss: -2.0203606011582287\n",
      "Loss: 0.2293104277321753\n",
      "Loss: -6.817911144789273\n",
      "Loss: -3.6539631233287055\n",
      "Loss: -0.659548542346075\n",
      "Loss: -1.8260595284082901\n",
      "Loss: -4.781882466157434\n",
      "Loss: -2.8308629808120993\n",
      "Loss: -2.753627957165212\n",
      "Loss: -1.1686360313174553\n",
      "Loss: -5.147598240313395\n",
      "Loss: -5.754984320366117\n",
      "Loss: -0.7189985346903531\n",
      "Loss: -2.2083513160978065\n",
      "Loss: 0.0023535742919157387\n",
      "Loss: -3.824189203178589\n",
      "Loss: -1.714472131896263\n",
      "Loss: -1.028137059275824\n",
      "Loss: -2.9511227867902257\n",
      "Loss: -2.7666345824066823\n",
      "Loss: 0.006445986729284703\n",
      "Loss: -0.10492089416106154\n",
      "Loss: -1.9861527835118613\n",
      "Loss: -6.435096155899979\n",
      "Loss: -1.2412212487853043\n",
      "Loss: -5.80320718054875\n",
      "Loss: 0.13553732087724352\n",
      "Loss: -3.987249502420822\n",
      "Loss: -3.58699972657726\n",
      "Loss: -1.4535550773650068\n",
      "Loss: -1.5893524928945935\n",
      "Loss: -4.992326111436842\n",
      "Loss: -4.364356097539399\n",
      "Loss: -0.36548545970481794\n",
      "Loss: -0.48080664486052094\n",
      "Loss: -1.220294651364334\n",
      "Loss: -4.130930459339152\n",
      "Loss: -0.5714371955127648\n",
      "Loss: -3.4622289367684385\n",
      "Loss: -6.401816403778574\n",
      "Loss: -3.358548981350031\n",
      "Loss: -5.1877536090273715\n",
      "Loss: 0.12724308883338664\n",
      "Loss: -5.809527820209386\n",
      "Loss: -2.8267420772986975\n",
      "Loss: -0.171956315831425\n",
      "Loss: -0.6662078462284085\n",
      "Loss: -3.7147189610450546\n",
      "Loss: -7.371348377629019\n",
      "Loss: -5.491044332292025\n",
      "Loss: -3.041782382498928\n",
      "Loss: -2.1688741435834973\n",
      "Loss: -1.0345206152326853\n",
      "Loss: -5.495765047131196\n",
      "Loss: -1.035597866871261\n",
      "Loss: 0.041998570538477775\n",
      "Loss: -9.794905044638345\n",
      "Loss: -2.277532495463688\n",
      "Loss: -0.16989419141344322\n",
      "Loss: -1.4328793550131302\n",
      "Loss: -0.2676140834561801\n",
      "Loss: -2.7297747445745983\n",
      "Loss: -4.714306877440519\n",
      "Loss: -1.1884613726221593\n",
      "Loss: -1.9549980429443785\n",
      "Loss: -1.0355014431497263\n",
      "Loss: 0.14747123707145587\n",
      "Loss: -2.304881193537526\n",
      "Loss: -0.6747435786959008\n",
      "Loss: -4.618748526131528\n",
      "Loss: -4.4932912613764415\n",
      "Loss: -0.424890349216093\n",
      "Loss: -5.338601375193882\n",
      "Loss: -3.9569744865036482\n",
      "Loss: 0.22467617456225747\n",
      "Loss: -0.6126373965344739\n",
      "Loss: -0.5199194014628132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -7.53960933169394\n",
      "Loss: -4.065350281871105\n",
      "Loss: -0.7183228610120682\n",
      "Loss: -4.600109803962276\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-0.9725, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: -0.9725284857619614\n",
      "Loss: -0.7730489533053886\n",
      "Loss: -1.4549103035036015\n",
      "Loss: -3.112540383138763\n",
      "Loss: -2.524271553838425\n",
      "Loss: -3.385123074610495\n",
      "Loss: 0.1596676132997224\n",
      "Loss: -4.442177838104133\n",
      "Loss: -1.215587121964959\n",
      "Loss: -1.2929480769289405\n",
      "Loss: -4.357722775258153\n",
      "Loss: -2.6211567214356504\n",
      "Loss: -2.4356640241301295\n",
      "Loss: -2.9390015366081155\n",
      "Loss: -1.6579007013259404\n",
      "Loss: -5.883294454214101\n",
      "Loss: -1.9713992081779588\n",
      "Loss: -4.681642510141437\n",
      "Loss: -2.73976785923219\n",
      "Loss: -4.083067758424608\n",
      "Loss: -1.2720282837356855\n",
      "Loss: -0.6353079548069025\n",
      "Loss: -1.5777035429827324\n",
      "Loss: -0.8870256956988924\n",
      "Loss: -1.8701039945469573\n",
      "Loss: -1.3800524371886982\n",
      "Loss: -0.44345495034442\n",
      "Loss: -2.1926339144988827\n",
      "Loss: -0.7751428165057193\n",
      "Loss: -6.916100232269057\n",
      "Loss: -0.3094946619189095\n",
      "Loss: -1.6527336951142557\n",
      "Loss: -3.602033927927534\n",
      "Loss: -3.6566170488273486\n",
      "Loss: -1.6561056473341664\n",
      "Loss: -1.8078015921916044\n",
      "Loss: -2.59464314601181\n",
      "Loss: -3.1295768498930574\n",
      "Loss: -1.5958193926038127\n",
      "Loss: -3.1209722507029\n",
      "Loss: -6.031534227239803\n",
      "Loss: -3.9852080992803236\n",
      "Loss: -5.327972537717087\n",
      "Loss: 0.22744367671193916\n",
      "Loss: -3.0253451391586097\n",
      "Loss: -0.8839399690928464\n",
      "Loss: 0.16632815783416693\n",
      "Loss: -5.203027077226699\n",
      "Loss: -0.20961646245910295\n",
      "Loss: -1.5307814553342478\n",
      "Loss: -2.0670847408125415\n",
      "Loss: -3.8268343740816175\n",
      "Loss: -2.246227615586317\n",
      "Loss: -0.4209073798665297\n",
      "Loss: -5.532492272184515\n",
      "Loss: -0.8880748656618349\n",
      "Loss: -2.527741817132007\n",
      "Loss: -7.277988181326228\n",
      "Loss: -0.39945532835321296\n",
      "Loss: -1.9386239355042512\n",
      "Loss: -4.005196785983175\n",
      "Loss: -2.103910110176466\n",
      "Loss: -6.580747739972845\n",
      "Loss: -4.419213427597502\n",
      "Loss: 0.2275669448784296\n",
      "Loss: -0.3017804887155708\n",
      "Loss: -3.414234686084337\n",
      "Loss: -7.696469586622838\n",
      "Loss: -3.5312441207469574\n",
      "Loss: 0.02988836029913894\n",
      "Loss: -0.6958901160559653\n",
      "Loss: -6.004280447948198\n",
      "Loss: -5.01208328128874\n",
      "Loss: -4.332351943260566\n",
      "Loss: -1.905832512162577\n",
      "Loss: -2.2728297045068984\n",
      "Loss: 0.2262610243121415\n",
      "Loss: -4.847580459403947\n",
      "Loss: -2.025946279115611\n",
      "Loss: -2.0759500200196603\n",
      "Loss: -5.186802881012486\n",
      "Loss: -0.9567792242996414\n",
      "Loss: -5.0383573202423255\n",
      "Loss: -3.9934916867828347\n",
      "Loss: -0.4988235963122345\n",
      "Loss: -1.59814117852925\n",
      "Loss: -6.607316254542276\n",
      "Loss: -1.117880478894287\n",
      "Loss: 0.19638376086977333\n",
      "Loss: -3.0461419748635037\n",
      "Loss: -2.6195547428680723\n",
      "Loss: -7.656232191433611\n",
      "Loss: -5.4007128183522015\n",
      "Loss: -1.8709729200602148\n",
      "Loss: -2.2546738854089097\n",
      "Loss: -0.42742386931152926\n",
      "Loss: -1.3998120384632737\n",
      "Loss: -0.6802085141967811\n",
      "Loss: -5.641414566255518\n",
      "Loss: -2.309735289546146\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-3.5917, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: -3.591692823695286\n",
      "Loss: -0.36506739024202134\n",
      "Loss: -1.193220003733782\n",
      "Loss: -1.5468343028574272\n",
      "Loss: -2.9315476798874194\n",
      "Loss: -2.084851727964416\n",
      "Loss: -3.986755847007872\n",
      "Loss: 0.21914811834439368\n",
      "Loss: -0.6306786143423806\n",
      "Loss: -2.088685691550976\n",
      "Loss: -1.7935573177751463\n",
      "Loss: -1.4114529986292144\n",
      "Loss: -4.310261433288466\n",
      "Loss: -0.15179305711357385\n",
      "Loss: 0.1862328875570975\n",
      "Loss: -1.8841498637325562\n",
      "Loss: -0.4945446821278827\n",
      "Loss: -0.01076725341806184\n",
      "Loss: -4.6720588131617635\n",
      "Loss: -3.38624449927766\n",
      "Loss: -1.1795350625855932\n",
      "Loss: -1.3104156118383978\n",
      "Loss: -0.7466215521750464\n",
      "Loss: -3.6188238076921246\n",
      "Loss: -2.0050746857123674\n",
      "Loss: -5.275548441936826\n",
      "Loss: -3.5751384325570035\n",
      "Loss: -3.274526161572355\n",
      "Loss: -5.098273987392916\n",
      "Loss: -1.0023822601103596\n",
      "Loss: -1.0734936005714455\n",
      "Loss: -0.812836145599812\n",
      "Loss: 0.12497327068936054\n",
      "Loss: -1.0172573103896798\n",
      "Loss: -0.36306024531652104\n",
      "Loss: -2.1150172678312242\n",
      "Loss: -4.873851564679714\n",
      "Loss: -3.123548683533649\n",
      "Loss: -3.3313251979397522\n",
      "Loss: -1.0174510006650912\n",
      "Loss: -0.8049128178339664\n",
      "Loss: -1.6406055219947528\n",
      "Loss: -1.8107523144027078\n",
      "Loss: -3.513682273140756\n",
      "Loss: -1.933801562727727\n",
      "Loss: -1.2557727948168615\n",
      "Loss: -0.6895630654076547\n",
      "Loss: -0.2257021760587097\n",
      "Loss: -1.345064786234974\n",
      "Loss: -4.801366674518345\n",
      "Loss: -0.7405276773442658\n",
      "Loss: -4.5094703683748145\n",
      "Loss: -3.0209495416385663\n",
      "Loss: -3.111391639078887\n",
      "Loss: -6.90907667865323\n",
      "Loss: -2.7260421200695615\n",
      "Loss: -4.582966377402378\n",
      "Loss: -2.8138783696342737\n",
      "Loss: -2.2255757149596276\n",
      "Loss: -3.964017926186037\n",
      "Loss: -3.3034474443676807\n",
      "Loss: -3.836230121931196\n",
      "Loss: -0.2405323775641249\n",
      "Loss: -0.08600435167146553\n",
      "Loss: -1.1597116696042111\n",
      "Loss: -0.887454021061578\n",
      "Loss: -0.5719840473414562\n",
      "Loss: -2.6419003460012274\n",
      "Loss: -2.117675776605859\n",
      "Loss: -7.468953522193909\n",
      "Loss: -1.9625512020310536\n",
      "Loss: 0.2109295527622202\n",
      "Loss: -2.404699659727824\n",
      "Loss: 0.06362821808474695\n",
      "Loss: -1.315831181036163\n",
      "Loss: -0.22765236389288948\n",
      "Loss: -1.253135411127737\n",
      "Loss: -4.859515316517626\n",
      "Loss: -3.3388750427300065\n",
      "Loss: -0.6474222408749155\n",
      "Loss: -0.3007352159636739\n",
      "Loss: -2.692038281409206\n",
      "Loss: -6.469794371542792\n",
      "Loss: -2.281831356671724\n",
      "Loss: -1.8692230456951497\n",
      "Loss: 0.04865239130847959\n",
      "Loss: -4.118011268212841\n",
      "Loss: -0.4921542457058303\n",
      "Loss: -0.5017323684041217\n",
      "Loss: -4.198644522716124\n",
      "Loss: -1.2672567520405424\n",
      "Loss: -1.44222736954904\n",
      "Loss: -0.9926238603105897\n",
      "Loss: -3.2382228907745887\n",
      "Loss: -4.769093430129327\n",
      "Loss: -0.5758956753133679\n",
      "Loss: 0.16449939047267065\n",
      "Loss: -6.835188925900314\n",
      "Loss: -1.683345272539272\n",
      "Loss: -2.286275196438055\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-1.9899, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: -1.989888419631069\n",
      "Loss: -2.355245247759242\n",
      "Loss: -3.1713633947119955\n",
      "Loss: -1.679576928126574\n",
      "Loss: -5.378130488394651\n",
      "Loss: -0.25211368780276916\n",
      "Loss: -0.2985610321796989\n",
      "Loss: -1.6635539473216383\n",
      "Loss: -4.734326957930698\n",
      "Loss: -2.6273183752304994\n",
      "Loss: -2.1893800738436675\n",
      "Loss: -4.047104647570455\n",
      "Loss: -3.5642558243275704\n",
      "Loss: -2.2156146619006076\n",
      "Loss: -2.0397679790566015\n",
      "Loss: -4.132422444382039\n",
      "Loss: -2.1166033915880607\n",
      "Loss: -2.3279665975502524\n",
      "Loss: -1.8829155130599065\n",
      "Loss: -1.8150152849036627\n",
      "Loss: -6.023126980635541\n",
      "Loss: -1.5290669152096075\n",
      "Loss: -3.969243561317047\n",
      "Loss: -3.7062699179108294\n",
      "Loss: -0.5171344272465432\n",
      "Loss: -7.638981001178485\n",
      "Loss: -0.42678180360734924\n",
      "Loss: -7.340979243117491\n",
      "Loss: -2.482345963885527\n",
      "Loss: -5.497419707114998\n",
      "Loss: -2.3780492017754034\n",
      "Loss: -3.352919697461474\n",
      "Loss: -0.7046982977639384\n",
      "Loss: -1.3963175135894808\n",
      "Loss: -2.2428510790952703\n",
      "Loss: -1.807139123699245\n",
      "Loss: 0.10694364671056544\n",
      "Loss: -0.7766242242758165\n",
      "Loss: -2.188709218137503\n",
      "Loss: -0.9250755946792946\n",
      "Loss: -3.9121686879578257\n",
      "Loss: -0.8293405593999319\n",
      "Loss: -0.0758081414055326\n",
      "Loss: -0.45815114219297726\n",
      "Loss: -5.23886509373653\n",
      "Loss: -0.801975811914975\n",
      "Loss: -1.3176277726598804\n",
      "Loss: -0.6681577892710595\n",
      "Loss: -0.3088465761923794\n",
      "Loss: -4.841452909732079\n",
      "Loss: -1.6236927715793266\n",
      "Loss: -1.9903543112393653\n",
      "Loss: -4.380448281532678\n",
      "Loss: -2.9687873101761695\n",
      "Loss: -0.9310486870503847\n",
      "Loss: -0.9769967297241928\n",
      "Loss: -1.6879442712759725\n",
      "Loss: -2.2465164405525067\n",
      "Loss: -2.684806056674316\n",
      "Loss: -0.9106028407994784\n",
      "Loss: -1.1481838342164548\n",
      "Loss: -1.9082134286416965\n",
      "Loss: -1.8112617447316937\n",
      "Loss: -4.0998530446482055\n",
      "Loss: -0.5732051923577206\n",
      "Loss: -5.771774753492578\n",
      "Loss: -4.522071141762301\n",
      "Loss: -1.9032485329615754\n",
      "Loss: -4.995231455970759\n",
      "Loss: -3.7956931544879455\n",
      "Loss: -1.4779398315743566\n",
      "Loss: -3.6474340015755495\n",
      "Loss: -1.8269635216870286\n",
      "Loss: -3.7993853926383445\n",
      "Loss: -0.31955681721810014\n",
      "Loss: -3.9946780675663938\n",
      "Loss: -2.9392423610192338\n",
      "Loss: -6.721227945923013\n",
      "Loss: -0.11179121260849328\n",
      "Loss: -2.4661623851907843\n",
      "Loss: -2.015705427409795\n",
      "Loss: -6.163824083144522\n",
      "Loss: -4.2321569898515445\n",
      "Loss: -0.6428148658851991\n",
      "Loss: -4.466827485276232\n",
      "Loss: -2.8138774021148\n",
      "Loss: -3.882239134146064\n",
      "Loss: -2.6264696685013864\n",
      "Loss: -5.118159277935252\n",
      "Loss: 0.043958950197187874\n",
      "Loss: -0.359180410567156\n",
      "Loss: -3.526132640259754\n",
      "Loss: -1.52193340510368\n",
      "Loss: -1.4768442505446129\n",
      "Loss: -5.2679792466108015\n",
      "Loss: -0.28257943826671905\n",
      "Loss: -4.330019175198218\n",
      "Loss: -2.655546492226903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -3.244263778587728\n",
      "Loss: -0.6668867519357036\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-0.1598, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: -0.1597559240144422\n",
      "Loss: 0.010310677296564819\n",
      "Loss: -0.5814815321642939\n",
      "Loss: -1.167081142022176\n",
      "Loss: -2.0135327143975044\n",
      "Loss: -3.0866075948370617\n",
      "Loss: -2.3316812292904987\n",
      "Loss: 0.08859219379785066\n",
      "Loss: -3.1416699659094807\n",
      "Loss: -5.392264153554474\n",
      "Loss: -0.02348908571788566\n",
      "Loss: -3.335533760572375\n",
      "Loss: -1.7075077687512146\n",
      "Loss: -0.2738634605285787\n",
      "Loss: -0.9059362497188681\n",
      "Loss: -0.7880567372301239\n",
      "Loss: -1.7263764090114073\n",
      "Loss: -1.2486435276205452\n",
      "Loss: -3.649316628772133\n",
      "Loss: -2.661795294598919\n",
      "Loss: -1.1322859428398602\n",
      "Loss: -1.7254924756994774\n",
      "Loss: -1.7550654449972183\n",
      "Loss: -4.92407532707834\n",
      "Loss: -1.8275289480789152\n",
      "Loss: -1.2683374342661844\n",
      "Loss: -4.179324611155601\n",
      "Loss: -2.4248406853521804\n",
      "Loss: -2.2405292902669007\n",
      "Loss: -0.5628722994174722\n",
      "Loss: -2.766030359796234\n",
      "Loss: -0.497158340638728\n",
      "Loss: -2.523171203961917\n",
      "Loss: -5.608112658418808\n",
      "Loss: -0.045675755281031905\n",
      "Loss: -1.6798151123289378\n",
      "Loss: -3.9315297680616053\n",
      "Loss: -0.809800722850719\n",
      "Loss: -3.7952450110596794\n",
      "Loss: -1.5378922435607845\n",
      "Loss: -4.3409485799603775\n",
      "Loss: -2.328439182010029\n",
      "Loss: -7.069773758832052\n",
      "Loss: -0.8139439218934816\n",
      "Loss: -6.775542489047999\n",
      "Loss: -8.852563746787903\n",
      "Loss: -0.20268454510879\n",
      "Loss: -2.910011017940897\n",
      "Loss: -5.909489471008807\n",
      "Loss: -2.506944131661999\n",
      "Loss: -1.4854157175642206\n",
      "Loss: -2.7583979708547886\n",
      "Loss: -3.6119443809676848\n",
      "Loss: -3.0157615367674753\n",
      "Loss: -3.213497865796214\n",
      "Loss: -1.98481718254722\n",
      "Loss: -0.00037452933519283027\n",
      "Loss: -2.4895326190744758\n",
      "Loss: 0.187078924419617\n",
      "Loss: -0.35262041912005243\n",
      "Loss: -4.6858616438718625\n",
      "Loss: -2.421801312824967\n",
      "Loss: -6.279072927721605\n",
      "Loss: -0.3515700588919611\n",
      "Loss: -0.47844729733978214\n",
      "Loss: -2.4708003781289256\n",
      "Loss: -2.8994493775814623\n",
      "Loss: -4.339743358351125\n",
      "Loss: -4.0106485569439645\n",
      "Loss: -2.2947617054977267\n",
      "Loss: -1.2079506821205157\n",
      "Loss: -3.527672830913752\n",
      "Loss: -0.9338574037846705\n",
      "Loss: -7.402680230864762\n",
      "Loss: -4.492673839809658\n",
      "Loss: -2.7952659355676355\n",
      "Loss: -0.6554376680511936\n",
      "Loss: -1.0529158666266516\n",
      "Loss: -1.1859921497919081\n",
      "Loss: -0.043374224700385045\n",
      "Loss: -7.488071362547929\n",
      "Loss: -2.4664557094683524\n",
      "Loss: -0.4385071441032883\n",
      "Loss: -0.12390852780458447\n",
      "Loss: -4.308248204787877\n",
      "Loss: -1.3586951515204935\n",
      "Loss: -3.9504181922661847\n",
      "Loss: -0.6492881429287887\n",
      "Loss: -2.9162639662555283\n",
      "Loss: -2.613768006390209\n",
      "Loss: -0.06284476509916692\n",
      "Loss: -1.2385148576888267\n",
      "Loss: -1.467248107526764\n",
      "Loss: -0.4975086592097804\n",
      "Loss: -1.059861582909643\n",
      "Loss: -0.11292622760948845\n",
      "Loss: -0.5842690187024284\n",
      "Loss: -2.8096374393074606\n",
      "Loss: -0.0359655268467064\n",
      "Loss: -1.9080452008969637\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-1.0782, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: -1.078187356527823\n",
      "Loss: -2.610098449893755\n",
      "Loss: -2.7871694203455832\n",
      "Loss: -2.2415251681697175\n",
      "Loss: 0.207455839729417\n",
      "Loss: -5.585499883047244\n",
      "Loss: -0.4041400999238503\n",
      "Loss: -4.703475152911301\n",
      "Loss: -4.485713997518212\n",
      "Loss: -3.8086957889853177\n",
      "Loss: -0.9990251061163362\n",
      "Loss: -5.768878128498092\n",
      "Loss: -2.039931695170287\n",
      "Loss: -2.5481137836778114\n",
      "Loss: -0.30513841745787157\n",
      "Loss: -4.726298091186144\n",
      "Loss: -3.5082817632048906\n",
      "Loss: -2.610353019412645\n",
      "Loss: 0.028062519173503997\n",
      "Loss: -4.8928778489427085\n",
      "Loss: -1.3218587748694504\n",
      "Loss: -0.2670875165805439\n",
      "Loss: -0.2884825462833911\n",
      "Loss: -0.7878843289263551\n",
      "Loss: -0.787137462131911\n",
      "Loss: -10.884733010550907\n",
      "Loss: 0.08504262733436468\n",
      "Loss: -8.761454091399116\n",
      "Loss: -0.06627085815658662\n",
      "Loss: -2.0597130797572327\n",
      "Loss: -4.697993289602595\n",
      "Loss: -4.580289866018724\n",
      "Loss: -0.6686190591444421\n",
      "Loss: -4.17184014802303\n",
      "Loss: -6.751887000799214\n",
      "Loss: -2.77764280665924\n",
      "Loss: -0.29395589997575045\n",
      "Loss: -1.9359162074126484\n",
      "Loss: -2.777629224109587\n",
      "Loss: -2.0600191601055986\n",
      "Loss: -3.1010756416373124\n",
      "Loss: -1.6397809390286482\n",
      "Loss: -2.2426062969923044\n",
      "Loss: -0.5485332913459045\n",
      "Loss: -2.3993473847140914\n",
      "Loss: -0.9954028244947084\n",
      "Loss: -1.8475102324362158\n",
      "Loss: -0.5689231945848046\n",
      "Loss: -3.6426780070763227\n",
      "Loss: -1.465650622675173\n",
      "Loss: 0.22162398030778058\n",
      "Loss: -3.3524213888258054\n",
      "Loss: -3.7228321592058693\n",
      "Loss: -0.027173431066723208\n",
      "Loss: -2.620423328354647\n",
      "Loss: -0.8099537037834021\n",
      "Loss: -1.4718585170281837\n",
      "Loss: -0.507396727861672\n",
      "Loss: -4.037082565671143\n",
      "Loss: -3.750043747338538\n",
      "Loss: -3.9238850323674566\n",
      "Loss: -0.8748749160600389\n",
      "Loss: -3.021743740872264\n",
      "Loss: -3.793985012314487\n",
      "Loss: -0.28239425837573245\n",
      "Loss: -3.67057301701459\n",
      "Loss: -1.0743992142380052\n",
      "Loss: -5.563327773979604\n",
      "Loss: -7.798898599371574\n",
      "Loss: -1.6554296657646395\n",
      "Loss: -1.3465873114422724\n",
      "Loss: -5.0366415505666025\n",
      "Loss: -0.1122042331711382\n",
      "Loss: -1.6525872004864517\n",
      "Loss: -2.8657911798925095\n",
      "Loss: -0.17823421236645956\n",
      "Loss: -0.8569388253429552\n",
      "Loss: -4.777246974845617\n",
      "Loss: -2.061589192912326\n",
      "Loss: -0.7613649250834512\n",
      "Loss: -6.626556163966471\n",
      "Loss: -1.2744586100525466\n",
      "Loss: -1.7318633142425364\n",
      "Loss: -3.366571361941774\n",
      "Loss: -9.352686643846043\n",
      "Loss: -3.110655415096065\n",
      "Loss: -1.792606249036261\n",
      "Loss: -2.740209246632787\n",
      "Loss: -0.8733707439985863\n",
      "Loss: -0.821667115224335\n",
      "Loss: -3.032321416044852\n",
      "Loss: -0.7773029835821926\n",
      "Loss: -0.5197635934674152\n",
      "Loss: -3.220243780611246\n",
      "Loss: -2.7385782376862555\n",
      "Loss: -0.4348489640479217\n",
      "Loss: -0.08059566623749714\n",
      "Loss: -3.9576221144636645\n",
      "Loss: 0.015270923027067174\n",
      "Loss: -6.032631712853131\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-4.4475, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: -4.447502028216676\n",
      "Loss: -4.9296017197484145\n",
      "Loss: -0.4519505767718819\n",
      "Loss: -3.5984938615072184\n",
      "Loss: -1.2088927670918366\n",
      "Loss: 0.12689875535153006\n",
      "Loss: -0.0984828160096306\n",
      "Loss: -2.7286677368691445\n",
      "Loss: -6.144842503357261\n",
      "Loss: -1.146078838775264\n",
      "Loss: -3.403293125878035\n",
      "Loss: -4.087100208593991\n",
      "Loss: -5.31045923339\n",
      "Loss: -0.25992191671958276\n",
      "Loss: -4.985733004794489\n",
      "Loss: -2.5976604713447458\n",
      "Loss: 0.1557756009035925\n",
      "Loss: -3.2969529120052568\n",
      "Loss: -1.0398358714132165\n",
      "Loss: 0.02078267023618663\n",
      "Loss: -4.085285725027418\n",
      "Loss: -0.2327896603715147\n",
      "Loss: -3.4301836940127592\n",
      "Loss: -0.9557193507304549\n",
      "Loss: -2.3891118464849455\n",
      "Loss: -3.346485003529398\n",
      "Loss: -4.457971140036402\n",
      "Loss: -3.7832701590838824\n",
      "Loss: -3.062720359605204\n",
      "Loss: -1.2733454503245476\n",
      "Loss: -0.03861962763840893\n",
      "Loss: -1.29861818784287\n",
      "Loss: -0.6843464891285436\n",
      "Loss: -0.6036997252995366\n",
      "Loss: -2.227035611228601\n",
      "Loss: -0.22918557984561969\n",
      "Loss: -1.377272855082428\n",
      "Loss: -0.6503276076075352\n",
      "Loss: -1.721468290669958\n",
      "Loss: -3.7002191957096424\n",
      "Loss: -1.052147269084868\n",
      "Loss: -1.2050772411167154\n",
      "Loss: -4.113130855868633\n",
      "Loss: -0.5187484172476142\n",
      "Loss: -0.8246656881511093\n",
      "Loss: -0.5892535360739117\n",
      "Loss: -1.9562587050795581\n",
      "Loss: -5.660960660418844\n",
      "Loss: 0.10328752473964664\n",
      "Loss: -0.920759677919718\n",
      "Loss: -2.6128335383944714\n",
      "Loss: -1.6609649604669359\n",
      "Loss: 0.2100914940318505\n",
      "Loss: -5.910877205508192\n",
      "Loss: -4.770614082909358\n",
      "Loss: -1.3833522872620725\n",
      "Loss: -4.307669984728982\n",
      "Loss: -6.220361033280336\n",
      "Loss: -1.7389720766746597\n",
      "Loss: -1.6234771771712828\n",
      "Loss: -3.1957865728866754\n",
      "Loss: -0.3560340478404013\n",
      "Loss: -2.03312632197562\n",
      "Loss: 0.13494463557134512\n",
      "Loss: 0.09676371764012842\n",
      "Loss: -2.0757761351740065\n",
      "Loss: -1.8723721591361762\n",
      "Loss: -1.6270634599256786\n",
      "Loss: -3.5546889386859855\n",
      "Loss: -3.3212838170358188\n",
      "Loss: -0.21490524872225888\n",
      "Loss: -0.28421849551501804\n",
      "Loss: -1.4251846992176593\n",
      "Loss: -1.1080162587182334\n",
      "Loss: -1.7263165123341702\n",
      "Loss: -4.338945786478187\n",
      "Loss: -0.3343927026329225\n",
      "Loss: -4.090870600460599\n",
      "Loss: -2.7724977284298085\n",
      "Loss: 0.19232163327501056\n",
      "Loss: -4.3475802421844865\n",
      "Loss: -2.7583051099185183\n",
      "Loss: 0.006935511225882096\n",
      "Loss: -2.812498930305587\n",
      "Loss: -5.7162205504544294\n",
      "Loss: -3.8010191160906053\n",
      "Loss: -3.5703440588863997\n",
      "Loss: -0.7884430717645501\n",
      "Loss: -1.8329343870986035\n",
      "Loss: -1.0877947082956105\n",
      "Loss: -3.519644058599917\n",
      "Loss: -3.3877881614954597\n",
      "Loss: 0.16688697912389722\n",
      "Loss: -0.5572346620468114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.06067297599146315\n",
      "Loss: 0.09764337039690724\n",
      "Loss: -5.8345717944611515\n",
      "Loss: -1.311474050722934\n",
      "Loss: -1.8012178005820625\n",
      "Loss: -4.702261358180888\n",
      "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
      "         0.3666]], dtype=torch.float64, requires_grad=True)\n",
      "tensor(-4.3519, dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "Loss: -4.351886718309626\n",
      "Loss: -0.053066775157621926\n",
      "Loss: 0.06659161194951013\n",
      "Loss: -1.0929101926556004\n",
      "Loss: -1.5217839337624137\n",
      "Loss: -2.10566118762961\n",
      "Loss: -2.6883806141191275\n",
      "Loss: -0.25481276223498417\n",
      "Loss: -1.2133888507977322\n",
      "Loss: -1.7347111573311762\n",
      "Loss: -2.3203510546434805\n",
      "Loss: -1.2403028736608661\n",
      "Loss: -0.20974991805660462\n",
      "Loss: -4.195536294927454\n",
      "Loss: -1.5398165150366268\n",
      "Loss: -0.7891494346169807\n",
      "Loss: -0.5687430806928695\n",
      "Loss: -5.832119874273461\n",
      "Loss: -4.365469838513782\n",
      "Loss: -0.6143241150391301\n",
      "Loss: -0.716679881060843\n",
      "Loss: -2.1952278670052237\n",
      "Loss: -3.181291023142376\n",
      "Loss: 0.02691723335683105\n",
      "Loss: -0.677520443927491\n",
      "Loss: -4.352896918903026\n",
      "Loss: -2.0050484998333835\n",
      "Loss: 0.10002230812733803\n",
      "Loss: -3.514224738314766\n",
      "Loss: -1.5708722276845895\n",
      "Loss: -1.0571661459326602\n",
      "Loss: -0.6974005190315972\n",
      "Loss: -5.917279084916304\n",
      "Loss: -0.24776016579182636\n",
      "Loss: -3.6088351804601375\n",
      "Loss: -4.461290113628715\n",
      "Loss: -1.2730997697488022\n",
      "Loss: -2.1155611183848424\n",
      "Loss: -1.2721012869436854\n",
      "Loss: -3.9875019694600313\n",
      "Loss: -1.6576070588548582\n",
      "Loss: -3.3571275331301424\n",
      "Loss: -2.185151983708004\n",
      "Loss: 0.20075196715685611\n",
      "Loss: -1.8790691768747902\n",
      "Loss: -4.791467147994785\n",
      "Loss: -2.283347005938028\n",
      "Loss: -0.08862409952628486\n",
      "Loss: 0.0926972910106939\n",
      "Loss: -0.7861224991633828\n",
      "Loss: -1.0196681399415801\n",
      "Loss: -2.4512962664122266\n",
      "Loss: -1.0557171570708361\n",
      "Loss: -1.4087678749417853\n",
      "Loss: -2.681848678085174\n",
      "Loss: -4.1567808029849855\n",
      "Loss: -1.7850095103748573\n",
      "Loss: -7.3729803815284525\n",
      "Loss: -1.0936799639296713\n",
      "Loss: -3.475292346048642\n",
      "Loss: -0.5402651091126638\n",
      "Loss: -1.8932311910469792\n",
      "Loss: -2.7969164033443428\n",
      "Loss: -0.7305999278106344\n",
      "Loss: -2.848856983803407\n",
      "Loss: -0.813118677726123\n",
      "Loss: -1.7030555954434918\n",
      "Loss: -2.870726536509744\n",
      "Loss: -0.5273770568025973\n",
      "Loss: -2.435333247952033\n",
      "Loss: -1.3930799317310945\n",
      "Loss: -7.833778918053422\n",
      "Loss: 0.0895954739897783\n",
      "Loss: -4.988815979593133\n",
      "Loss: -2.7812372365741176\n",
      "Loss: -1.7977924730603991\n",
      "Loss: -4.438265285794237\n",
      "Loss: -6.423013662919415\n",
      "Loss: -0.7835038338892967\n",
      "Loss: -1.6388220464213916\n",
      "Loss: -2.525896856938654\n",
      "Loss: -3.2150467177499658\n",
      "Loss: -1.7103686553098212\n",
      "Loss: -4.144908380375883\n",
      "Loss: -1.5920719022154144\n",
      "Loss: -2.9895857742823346\n",
      "Loss: -1.495665635894152\n",
      "Loss: -1.607824210127869\n",
      "Loss: -4.425607884742721\n",
      "Loss: -1.0270198997561812\n",
      "Loss: -0.09383977441076824\n",
      "Loss: -6.256822747770738\n",
      "Loss: -1.9876387559912798\n",
      "Loss: -1.1478177992774243\n",
      "Loss: -2.4072677475671482\n",
      "Loss: -3.148394078764578\n",
      "Loss: -0.1519097940215862\n",
      "Loss: -2.48468801041435\n",
      "Loss: -4.737137214632787\n",
      "Loss: -0.7603087881294516\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_iters):\n",
    "    for _ in range(n_sub_iters):\n",
    "        # sample x\n",
    "        x = normal_dist.sample()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer_map.zero_grad()\n",
    "        \n",
    "        # Get dual objective to maximise\n",
    "        dual_objective = er_ctran(x, g, y, epsilon, l2_cost)\n",
    "        map_loss = -dual_objective\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        map_loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer_map.step()\n",
    "        \n",
    "    for _ in range(n_sub_iters):\n",
    "        # sample x\n",
    "        x = normal_dist.sample()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer_atoms.zero_grad()\n",
    "        \n",
    "        # Get loss objective to minimise\n",
    "        atoms_loss = er_ctran(x, g, y, epsilon, l2_cost)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        atoms_loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer_atoms.step()\n",
    "    \n",
    "    # Updating parameters\n",
    "    if i % 100 == 0:\n",
    "        print(g)\n",
    "        print(map_loss)\n",
    "        # Decay Learning Rate\n",
    "        scheduler_atoms.step()\n",
    "        scheduler_map.step()\n",
    "    \n",
    "    print(\"Loss: {0}\".format(map_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666, 0.3666,\n",
       "         0.3666]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0429, 0.0429, 0.0429, 0.0429, 0.0429, 0.0429, 0.0429, 0.0429, 0.0429,\n",
       "        0.0429], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
